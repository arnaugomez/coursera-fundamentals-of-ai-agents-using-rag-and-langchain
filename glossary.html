<html>
  <head>
    <style>
      .linenums {
        list-style-type: none;
      }

      .formatted-line-numbers {
        display: none;
      }
      .action-code-block {
        display: none;
      }
      table {
        border-collapse: collapse;
        width: 100%;
      }
      table,
      th,
      td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
      }
    </style>
  </head>
  <body>
    <div>
      <h1>
        <span class="header-link octicon octicon-link"></span>Glossary:
        Fundamentals of Building AI Agents using RAG and LangChain
      </h1>
      <div>
        <p>
          Welcome! This alphabetized glossary contains many terms used in this
          course. Understanding these terms is essential when working in the
          industry, participating in user groups, and participating in other
          certificate programs.
        </p>
        <p>Estimated reading time: 5 minutes</p>
        <table>
          <thead>
            <tr>
              <th>Term</th>
              <th>Definition</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Bidirectional and Auto-Regressive Transformers (BART)</td>
              <td>
                Sequence-to-sequence large language model (LLM) that follows an
                encoder-decoder architecture. It leverages encoding for
                contextual understanding and decoding to generate text.
              </td>
            </tr>
            <tr>
              <td>Bidirectional Representation of Transformers (BERT)</td>
              <td>
                An open-source, deeply bidirectional, unsupervised language
                representation pretrained using a plain text corpus.
              </td>
            </tr>
            <tr>
              <td>Bradley-Terry model</td>
              <td>
                A probability model for the outcome of pairwise comparisons
                between items, teams, or objects.
              </td>
            </tr>
            <tr>
              <td>Chain-of-thought (CoT)</td>
              <td>
                An AI technique that simulates human-like reasoning by breaking
                down complex tasks into logical steps.
              </td>
            </tr>
            <tr>
              <td>Chat model</td>
              <td>
                A model designed for efficient conversations. It means that it
                understands the questions or prompts and responds to them like a
                human.
              </td>
            </tr>
            <tr>
              <td>Context encoder</td>
              <td>A neural network architecture used for image inpainting.</td>
            </tr>
            <tr>
              <td>Contextual embeddings</td>
              <td>
                A type of embedding that aptly describes how the transformer
                processes the input word embeddings by accounting for the
                context in which each word occurs within the sequence.
              </td>
            </tr>
            <tr>
              <td>Data leakage</td>
              <td>
                An organization faces challenges in exposing sensitive
                information.
              </td>
            </tr>
            <tr>
              <td>Dense Passage Retrieval (DPR)</td>
              <td>
                A set of tools that fetches relevant passages with respect to
                the question asked based on the similarity between the
                high-quality, low-dimensional continuous representation of
                passages and questions.
              </td>
            </tr>
            <tr>
              <td>Facebook AI Similarity Search (Faiss)</td>
              <td>
                It is a library developed by Facebook AI Research that offers
                efficient algorithms for searching through large collections of
                high-dimensional vectors.
              </td>
            </tr>
            <tr>
              <td>Faiss index</td>
              <td>
                A data structure that facilitates efficient similarities between
                vector searches.
              </td>
            </tr>
            <tr>
              <td>Few-shot prompt</td>
              <td>
                A technique where the model provides a small number of examples,
                usually between two and five, to adapt new examples from the
                previous objects.
              </td>
            </tr>
            <tr>
              <td>Fine-tuning</td>
              <td>
                A supervised process that optimizes the initially trained GPT
                model for specific tasks, like QA classification.
              </td>
            </tr>
            <tr>
              <td>Generative pre-trained transformer (GPT)</td>
              <td>
                A self-supervised model that involves training a decoder to
                predict the subsequent token or word in a sequence.
              </td>
            </tr>
            <tr>
              <td>GitHub</td>
              <td>
                A developer platform to create, store, manage, and share codes.
              </td>
            </tr>
            <tr>
              <td>Graphic processing unit (GPU)</td>
              <td>A process that helps to render graphic smoothly.</td>
            </tr>
            <tr>
              <td>Hugging Face</td>
              <td>
                Platform that offers an open-source library with pretrained
                models and tools to streamline the process of training and
                fine-tuning generative AI models.
              </td>
            </tr>
            <tr>
              <td>In-Context learning</td>
              <td>
                A technique in which task demonstrations are integrated into the
                prompt in a natural language format.
              </td>
            </tr>
            <tr>
              <td>LangChain</td>
              <td>
                An open-source interface that simplifies the application
                development process using LLMs. It facilitates a structured way
                to integrate language models into various use cases, including
                natural language processing or NLP and data retrieval.
              </td>
            </tr>
            <tr>
              <td>LangChain-Core</td>
              <td>
                A LangChain Expression Language and is the base for
                abstractions.
              </td>
            </tr>
            <tr>
              <td>LangChain chains</td>
              <td>Sequences of calls</td>
            </tr>
            <tr>
              <td>Language model</td>
              <td>
                A model that predicts words by analyzing the previous text,
                where context length acts as a hyperparameter.
              </td>
            </tr>
            <tr>
              <td>Large language models (LLMs)</td>
              <td>
                Foundation models that use AI and deep learning with vast data
                sets to generate text, translate languages, and create various
                types of content. They are called large language models due to
                the size of the training data set and the number of parameters.
              </td>
            </tr>
            <tr>
              <td>Machine learning</td>
              <td>
                Machine learning is a data analysis method for automating
                analytical model building.
              </td>
            </tr>
            <tr>
              <td>Model inference</td>
              <td>
                In machine learning, model inference refers to the
                operationalization of a trained ML model.
              </td>
            </tr>
            <tr>
              <td>Natural language processing (NLP)</td>
              <td>
                The subfield of artificial intelligence (AI) that deals with the
                interaction of computers and humans in human language. It
                involves creating algorithms and models that will help computers
                understand and comprehend human language and generate
                contextually relevant text in human language.
              </td>
            </tr>
            <tr>
              <td>Prompt engineering</td>
              <td>
                A process of creating effective prompts to enable AI models to
                generate responses based on the given inputs.
              </td>
            </tr>
            <tr>
              <td>Prompt template</td>
              <td>
                A predefined structure or a format that can be filled with
                specific content to generate prompts.
              </td>
            </tr>
            <tr>
              <td>Python</td>
              <td>A programming language.</td>
            </tr>
            <tr>
              <td>PyTorch</td>
              <td>
                A software-based open-source deep learning framework used to
                build neural networks, combining Torch's machine learning
                library with a Python-based high-level API.
              </td>
            </tr>
            <tr>
              <td>PyTorch tensors</td>
              <td>
                A fundamental data structure that is useful to represent a
                multi-dimensional array.
              </td>
            </tr>
            <tr>
              <td>Retrieval-augmented generation (RAG)</td>
              <td>
                RAG is an AI framework that helps optimize the output of large
                language models or LLMs. RAG uses the capabilities of LLMs in
                specific domains or the internal database of an organization
                without retraining the model.
              </td>
            </tr>
            <tr>
              <td>Scoring function</td>
              <td>
                Measures the summary for the evaluation of the point prediction.
                It means it predicts a property or a function.
              </td>
            </tr>
            <tr>
              <td>Self-consistency</td>
              <td>
                A technique for enhancing the reliability and accuracy of
                outputs.
              </td>
            </tr>
            <tr>
              <td>Tokenization</td>
              <td>
                The process of converting the words in the prompt into tokens.
              </td>
            </tr>
            <tr>
              <td>Text classifier</td>
              <td>
                A machine learning technique that assigns a set of predefined
                categories to open-ended text.
              </td>
            </tr>
            <tr>
              <td>Vector averaging</td>
              <td>Process of calculating mean vector from a set of vectors.</td>
            </tr>
            <tr>
              <td>watsonx.ai</td>
              <td>
                A platform that allows developers to leverage a wide range of
                large language models (LLMs) under IBM's own series.
              </td>
            </tr>
            <tr>
              <td>WatsonxLLM</td>
              <td>A wrapper of IBM watsonx.ai foundation models.</td>
            </tr>
            <tr>
              <td>Zero-shot prompt</td>
              <td>
                A prompt in natural language processing (NLP) where a model can
                generate results for tasks that have not been trained
                explicitly.
              </td>
            </tr>
          </tbody>
        </table>

        <p>
          <img
            src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZzqKzMYvDxlItsE7xSlAXw.png"
            alt=""
          />
        </p>
      </div>
    </div>
  </body>
</html>
